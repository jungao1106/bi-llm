{
    "model_name_or_path": "/data/gj/models/vicuna-7b-v1.3",
    "dataset_name": "imdb",
    "task": "nlu",
    "llama_based": true,
    "require_template": true,
    "require_inv": true,
    "num_labels": 2,
    "train_file": "/data/gj/data/imdb/train.json",
    "validation_file": "//data/gj/data/imdb/test.json",
    "test_file": "/data/gj/data/imdb/test.json",
    "overwrite_cache": false,
    "ddp_find_unused_parameters": false,
    "gradient_checkpointing": false,
    "overwrite_output_dir": true,
    "save_safetensors": true,
    "output_dir": "/data/gj/Bi-Attention-HFTrainer/output_dir/naive-vicuna-bi-nlu",
    "lora_config": null,
    "num_beams":4,
   
    "seed": 1106,
    "do_train": true,
    "do_eval": true,
    "do_predict": true,

    "ignore_pad_token_for_loss": true,
    "pad_to_max_length": true,

    "predict_with_generate": true,
    "prediction_loss_only": false,
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 1,
    "learning_rate": 3e-5,
    "generation_max_length": 32,
    "bf16": true,
    "fp16": false,


    "save_strategy": "steps",
    "evaluation_strategy": "steps",
    "metric_for_best_model": "loss",
    "load_best_model_at_end": true,
    "greater_is_better": false,
    "num_train_epochs": 10,
    "eval_steps": 900,
    "save_steps": 900,
    "max_train_samples": 999999,
    "max_eval_samples": 999999,
    "max_predict_samples": 999999,
    "max_source_length": 256,
    "max_target_length": 1,
    "report_to": "tensorboard",

    "remove_unused_columns": true,
    "require_lora": false,
    "lora_rank": 8,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_target_modules": ["k_proj", "v_proj"], 
    "lora_bias": "none"

}